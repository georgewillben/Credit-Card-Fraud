{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "* Load data\n",
    "* Tune hyper parameters for each version of the data\n",
    "* Select a model\n",
    "* Examine results\n",
    "* Save results\n",
    "\n",
    "## Tools Used\n",
    "* Pickle\n",
    "* Numpy\n",
    "* Pandas\n",
    "* Matplotlib\n",
    "* Sklearn\n",
    "* Imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import time\n",
    "from modeling_functions import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((254968, 53), (28330, 53))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "pickle_in = open(\"engineered_data.pickle\", \"rb\")\n",
    "df = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "# Seperate X and y\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df.Class\n",
    "\n",
    "# stratified train test split\n",
    "train_index, test_index = next(StratifiedShuffleSplit(test_size=0.1).split(X,y))\n",
    "X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('s1_r1_o1', 19), ('s1_r1_o3', 18), ('s1_r2_o1', 17), ('s1_r2_o3', 16), ('s2_r1_o1', 26), ('s2_r1_o3', 26), ('s2_r2_o1', 21), ('s2_r2_o3', 22), ('s3_r1_o1', 23), ('s3_r1_o3', 25), ('s3_r2_o1', 17), ('s3_r2_o3', 20)]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "pickle_in = open(\"olr_keys_n_components.pickle\", \"rb\")\n",
    "olr_keys_n_components = list(pickle.load(pickle_in))\n",
    "pickle_in.close()\n",
    "\n",
    "# Sanity Check\n",
    "print(olr_keys_n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My goal for this project is to create a model that can help alert a credit lender to suspicious activity. <br><br>\n",
    "\n",
    "For this reason I want to have low false negatives, so I will be using recall as my main metric. High recall will mean a low amount of fradulent transactions are left undetected. <br><br>\n",
    "\n",
    "My second metric will be precision because I do not want false positives either. low precision would cause the model to flag too large an amount of the data as likely to be fraudulent. If the credit lender chose to take preventative action on say, every other transaction, then that would be a nuisance to both the credit lender and the clients. <br><br>\n",
    "\n",
    "However precision does not need to be nearly as high as recall. If recall was say %80 then I would have potentially stopped %80 percent of fraud and if precision was say %20 then less than 1 out of 100 transactions would be flagged as suspicous to fraud, because in this dataset fraud accounts for %0.17 percent of the data I was  given. <br><br> \n",
    "\n",
    "F1-score is the harmonic mean of recall and precision. It is not the best metric to use though because it is important that recall is high, but precision can get away with being much lower.<br><br>\n",
    "\n",
    "The metrics mentioned above are calculated by comparing the known values to the model's predicted values. The simplified formulas for precision and recall are showed below. \n",
    "<img src=\"../Images/Precision_Recall.png\"><br>\n",
    "<a href=\"https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\">Image Source</a> \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tune the hyperparameters I will use my own function called customGridSearch. It has a doc string attached. The function will go through the data transforming it according to the function's parameters and return the cross validation scores for each method as well as for each combination of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter grid in the cell below was inspired by the <a href=https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets>top rated kaggle post</a> I marked the url as source 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instantiate logistic regression classifer\n",
    "clf = LogisticRegression\n",
    "\n",
    "# Create parameter grid (Source 1)\n",
    "params = {\n",
    "    \"penalty\": [\"l1\", \"l2\"], \n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    \"solver\": [\"liblinear\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning the following cell 5 took minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Standard ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Robust ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "17.543113509813946\n"
     ]
    }
   ],
   "source": [
    "# Tune hyperparameters for all scalers \n",
    "# Implementing NearMiss\n",
    "# No outliers removed\n",
    "# No PCA\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Record results\n",
    "results_o1 = {}\n",
    "scaler_str = [\"Min-Max\", \"Standard\", \"Robust\"]\n",
    "for n, scaler in enumerate([MinMaxScaler(), StandardScaler(), RobustScaler()]):\n",
    "    print(scaler_str[n], '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    results_o1[scaler_str[n]] = customGridSearchCV(clf, params, X_train, y_train, 'custom', scaler, NearMiss())\n",
    "    \n",
    "t2 = time.time()\n",
    "\n",
    "print((t2 - t1)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: The following cell took 4 minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      " {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'} \n",
      " recall 0.6679728317659352 \n",
      " precision: 0.6396700632438357 \n",
      " f1-score: 0.5593222783287597 \n",
      "\n",
      "Standard ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Robust ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "17.16404824256897\n"
     ]
    }
   ],
   "source": [
    "# Tune hyperparameters for all scalers \n",
    "# Implementing NearMiss\n",
    "# outliers removed\n",
    "# No PCA\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Record results\n",
    "results_o3 = {}\n",
    "scaler_str = [\"Min-Max\", \"Standard\", \"Robust\"]\n",
    "for n, scaler in enumerate([MinMaxScaler(), StandardScaler(), RobustScaler()]):\n",
    "    print(scaler_str[n], '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    results_o3[scaler_str[n]] = customGridSearchCV(clf, params, X_train, y_train, 'custom', scaler, NearMiss(), outlier_removal=True)\n",
    "    \n",
    "t2 = time.time()\n",
    "\n",
    "print((t2 - t1)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: The following cell took 8 minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Standard ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Robust ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "17.133947738011678\n"
     ]
    }
   ],
   "source": [
    "# Tune hyperparameters for all scalers \n",
    "# Implementing NearMiss\n",
    "# No outliers removed\n",
    "# PCA\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "# Get correct scaler_str's and n_components (for PCA)\n",
    "o1_n_components = []\n",
    "for key, n in olr_keys_n_components:\n",
    "    if 'r2_o1' in key:\n",
    "        o1_n_components.append(n)\n",
    "        \n",
    "\n",
    "# Record results\n",
    "results_o1_p = {}\n",
    "scaler_str = [\"Min-Max\", \"Standard\", \"Robust\"]\n",
    "for n, scaler, k in zip([0,1,2],[MinMaxScaler(), StandardScaler(), RobustScaler()], o1_n_components):\n",
    "    print(scaler_str[n], '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    results_o1_p[scaler_str[n]] = customGridSearchCV(clf, params, X_train, y_train, 'custom', scaler, NearMiss(), pca=PCA(k))\n",
    "t2 = time.time()\n",
    "\n",
    "print((t2 - t1)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: this cell took 8 minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      " {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'} \n",
      " recall 0.7505224660397074 \n",
      " precision: 0.530077052560172 \n",
      " f1-score: 0.5118384578980918 \n",
      "\n",
      "Standard ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      " {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'} \n",
      " recall 0.7941222570532915 \n",
      " precision: 0.4868455336966675 \n",
      " f1-score: 0.5284148094658674 \n",
      "\n",
      "Robust ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      " {'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'} \n",
      " recall 0.7321577847439916 \n",
      " precision: 0.8283664597735001 \n",
      " f1-score: 0.7745360787125343 \n",
      "\n",
      "17.110976183414458\n"
     ]
    }
   ],
   "source": [
    "# Tune hyperparameters for all scalers \n",
    "# Implementing NearMiss\n",
    "# Outliers removed\n",
    "# PCA\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "# Get correct scaler_str's and n_components (for PCA)\n",
    "o3_n_components = []\n",
    "for key, n in olr_keys_n_components:\n",
    "    if 'r2_o3' in key:\n",
    "        o3_n_components.append(n)\n",
    "        \n",
    "\n",
    "# Record results\n",
    "results_o1_p = {}\n",
    "for n, scaler, k in zip([0,1,2],[MinMaxScaler(), StandardScaler(), RobustScaler()], o3_n_components):\n",
    "    print(scaler_str[n], '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    results_o1_p[scaler_str[n]] = customGridSearchCV(clf, params, X_train, y_train, 'custom', scaler, NearMiss(), \n",
    "                                                     outlier_removal=True, pca=PCA(k))\n",
    "    \n",
    "t2 = time.time()\n",
    "\n",
    "print((t2 - t1)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will look through each models scores manually and conclude which one is the best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My choice a model using the data scaled with StandardScaler, with the outliers removed and with PCA because it had a high cross validated recall of ~%79 and precision of ~%49\n",
    "The models parameters are as follows: <br> <br>\n",
    "C: 0.1<br>\n",
    "penalty: \"l1\"<br>\n",
    "solver: \"liblinear\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will check the scores at each split of the model to make sure it is not over fitting to a specific split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 1\n",
      "recall: 0.8736\n",
      "precision: 0.0987\n",
      "f1: 0.1774\n",
      "split 2\n",
      "recall: 0.7955\n",
      "precision: 0.5072\n",
      "f1: 0.6195\n",
      "split 3\n",
      "recall: 0.7273\n",
      "precision: 0.8649\n",
      "f1: 0.7901\n",
      "split 4\n",
      "recall: 0.8046\n",
      "precision: 0.5036\n",
      "f1: 0.6195\n",
      "split 5\n",
      "recall: 0.8391\n",
      "precision: 0.5034\n",
      "f1: 0.6293\n",
      "Mean Scores:\n",
      "Mean recall: 0.808\n",
      "Mean precision: 0.4956\n",
      "Mean f1: 0.5671 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8079937304075235, 0.49557158770839704, 0.5671469497061382]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best model\n",
    "# Print cross val scores\n",
    "for key, n in olr_keys_n_components:\n",
    "    if key == 's2_r2_o3':\n",
    "        n_components = n\n",
    "\n",
    "model = LogisticRegression(penalty=\"l1\", C=0.1, solver=\"liblinear\")\n",
    "customCV(model, X_train, y_train, StandardScaler(), NearMiss(), outlier_removal=True,\n",
    "         pca=PCA(n_components), print_splits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears to not be overfitting to a particular split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on test data\n",
    "Now I will run the model on the test data which has yet to be seen by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     28281\n",
      "           1       0.91      0.63      0.75        49\n",
      "\n",
      "    accuracy                           1.00     28330\n",
      "   macro avg       0.96      0.82      0.87     28330\n",
      "weighted avg       1.00      1.00      1.00     28330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit model to entire train set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# run model on test set\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# get results\n",
    "print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing scores are %63 recall and %91 precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will save the data along with a string to represent the transformations done to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"Models/LogReg.pickle\", \"wb\")\n",
    "pickle.dump([model, 's1_r2_o1, pca'+str(n_components)], pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "1) Top rated kaggle post: https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
